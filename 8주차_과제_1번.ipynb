{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMSAAYuWl7lbkXBluSRbU/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc07da7790ae466e8eb3665de735007f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e3c7bbc76b745a4bb8c89292437c03d",
              "IPY_MODEL_a18c77eb018346208e3291f4e3fcbf3e",
              "IPY_MODEL_eef895f983f24fe7818a776f0c26bf22"
            ],
            "layout": "IPY_MODEL_fc8836fd22c0419192d09bad90ad439c"
          }
        },
        "6e3c7bbc76b745a4bb8c89292437c03d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a472a22a64b640a3a6ab47ef7e15d193",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_af86f98289b2464e82c7933f14f1f072",
            "value": "Map:â€‡100%"
          }
        },
        "a18c77eb018346208e3291f4e3fcbf3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_942752f2444c4decbcf5c9ba2bf20966",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7a5a1b7b7554a299ced688548d7456d",
            "value": 4
          }
        },
        "eef895f983f24fe7818a776f0c26bf22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b1b8f1faaf44af19842f9375e2952b7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_aa547a04a4f541468587fa5614cf1ed2",
            "value": "â€‡4/4â€‡[00:00&lt;00:00,â€‡248.99â€‡examples/s]"
          }
        },
        "fc8836fd22c0419192d09bad90ad439c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a472a22a64b640a3a6ab47ef7e15d193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af86f98289b2464e82c7933f14f1f072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "942752f2444c4decbcf5c9ba2bf20966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7a5a1b7b7554a299ced688548d7456d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b1b8f1faaf44af19842f9375e2952b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa547a04a4f541468587fa5614cf1ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6729d9d8f4e47d9a7792f0946e60042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c745b50183db455d9df4d36fcf0b48d5",
              "IPY_MODEL_736ba1c3da644b3f9519fc3c6b617e3c",
              "IPY_MODEL_8373a97c6b5d4959b6c4b673329cad10"
            ],
            "layout": "IPY_MODEL_a8763fc044d24660948f3af96232bbd3"
          }
        },
        "c745b50183db455d9df4d36fcf0b48d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e5e1004a2544eba694fd4a762c389c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2831879a63ea45879bc896964677ed03",
            "value": "Map:â€‡100%"
          }
        },
        "736ba1c3da644b3f9519fc3c6b617e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_583bb5bc52d548a7be2865cf02513106",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0f8ff5dd101404cb62bdc1f4e56b28f",
            "value": 2
          }
        },
        "8373a97c6b5d4959b6c4b673329cad10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_859ec16fb4fa4c57b0c94a3dc4769d32",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_36f27c8bd52d46aebe2526afa7c110ca",
            "value": "â€‡2/2â€‡[00:00&lt;00:00,â€‡129.65â€‡examples/s]"
          }
        },
        "a8763fc044d24660948f3af96232bbd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39e5e1004a2544eba694fd4a762c389c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2831879a63ea45879bc896964677ed03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "583bb5bc52d548a7be2865cf02513106": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0f8ff5dd101404cb62bdc1f4e56b28f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "859ec16fb4fa4c57b0c94a3dc4769d32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36f27c8bd52d46aebe2526afa7c110ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoondonggyu/KTB-Project/blob/main/%EA%B3%BC%EC%A0%9C_1%EB%B2%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ìžì‹ ì´ ìƒê°í•˜ëŠ” ê°€ìž¥ ê¹”ë”í•œ íŠ¸ëžœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ íŒŒì´í† ì¹˜ë¡œ êµ¬í˜„í•˜ê³  ë¦¬ì–¼í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµê³¼ ì¶”ë¡ ì„ í•´ë³´ì„¸ìš”.\n",
        "\n",
        "2. 1ë²ˆì˜ ì½”ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ íŠ¸ëžœìŠ¤í¬ë¨¸ì˜ ì–¼ê°œë¥¼ ì„¤ëª…í•˜ëŠ” ë³´ê³ ì„œë¥¼ ìž‘ì„±í•´ë³´ì„¸ìš”.\n"
      ],
      "metadata": {
        "id": "5EpVANBsNPok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch ë‚´ìž¥(nn.Transformer)\n",
        "\n",
        "* ëª¨ë¸ : PyTorchì˜ torch.nn.Transformer í´ëž˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ëžœìŠ¤í¬ë¨¸ êµ¬ì¡°(ì¸ì½”ë”-ë””ì½”ë”)ë¥¼ ë‹¨ í•œ ì¤„ë¡œ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "* ìž¥ì \n",
        "   * ì½”ë“œì˜ ê¸¸ì´ê°€ ê·¹ë„ë¡œ ì§§ê³  ê¹”ë”í•©ë‹ˆë‹¤.\n",
        "   * ì‹¤ì œ ì—°êµ¬ë‚˜ ê°œë°œì—ì„œ íŠ¸ëžœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë¥¼ ë¹ ë¥´ê²Œ í™œìš©í•  ë•Œ ê°€ìž¥ ë§Žì´ ì“°ëŠ” ë°©ë²•\n",
        "   * íŒŒë¼ë¯¸í„° ì„¤ì •ë§Œìœ¼ë¡œ ì¸ì½”ë”/ë””ì½”ë” ê°œìˆ˜ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìžˆìŒ\n",
        "\n",
        "* ë‹¨ì \n",
        "  * íŠ¸ëžœìŠ¤í¬ë¨¸ì˜ ë‚´ë¶€ ë™ìž‘(Self-Attention , LayerNorm ë“±)ì„ ì§ì ‘ êµ¬í˜„í•´ë³´ëŠ” ê²½í—˜ì€ í•  ìˆ˜ ì—†ìŒ\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "-FcPeUxamlbi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0T3XDgR6K7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmTX_okTk3hk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # ðŸŒŸ í•µì‹¬: nn.TransformerEncoderLayerì™€ nn.TransformerEncoderë¥¼ ì‚¬ìš©\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "\n",
        "        # ìž…ë ¥(í† í°)ì„ d_model í¬ê¸°ì˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ìž„ë² ë”© ë ˆì´ì–´\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # ìµœì¢… ì¶œë ¥ì„ ntoken (ë‹¨ì–´/í† í° ê°œìˆ˜)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì„ í˜• ë ˆì´ì–´ (ì¶œë ¥ ë ˆì´ì–´)\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        # 1. ìž„ë² ë”©: ìž…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "\n",
        "        # 2. í¬ì§€ì…”ë„ ì¸ì½”ë”©: ì‹œí€€ìŠ¤ ìˆœì„œ ì •ë³´ ì¶”ê°€\n",
        "        src = self.pos_encoder(src)\n",
        "\n",
        "        # 3. íŠ¸ëžœìŠ¤í¬ë¨¸ ì¸ì½”ë”: í•µì‹¬ ì—°ì‚° ìˆ˜í–‰\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "\n",
        "        # 4. ì¶œë ¥ ë ˆì´ì–´: ìµœì¢… ê²°ê³¼ë¥¼ í† í° í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "# í•„ìˆ˜ êµ¬ì„± ìš”ì†Œ: í¬ì§€ì…”ë„ ì¸ì½”ë”© (Positional Encoding) ì •ì˜\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ í¬ê¸°, ìž„ë² ë”© ì°¨ì›] í˜•íƒœ\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "BOS_TOKEN = 1  # Start Of Sentence í† í°\n",
        "EOS_TOKEN = 2  # End Of Sentence í† í°\n",
        "PAD_TOKEN = 0  # Padding í† í°\n",
        "NTokens = 10   # ì „ì²´ í† í°(ë‹¨ì–´) ê°œìˆ˜ (0~9)\n",
        "EmbdSize = 256 # ìž„ë² ë”© ì°¨ì›\n",
        "NHead = 4      # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ í—¤ë“œ ê°œìˆ˜\n",
        "NHid = 256     # í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ì˜ ì€ë‹‰ ì°¨ì›\n",
        "NLayers = 2    # íŠ¸ëžœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ê°œìˆ˜\n",
        "SeqLen = 5     # ì‹œí€€ìŠ¤ ê¸¸ì´\n",
        "BatchSize = 32\n",
        "\n",
        "# ê°€ìƒ ë°ì´í„° ìƒì„± í•¨ìˆ˜\n",
        "def generate_batch(batch_size, seq_len, n_tokens):\n",
        "    # [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ í¬ê¸°] í˜•íƒœë¡œ ìƒì„± (PyTorch TransformerëŠ” ì´ í˜•íƒœë¥¼ ì„ í˜¸)\n",
        "    data = torch.randint(3, n_tokens, (seq_len, batch_size)) # 3~9 ì‚¬ì´ì˜ í† í°\n",
        "\n",
        "    # ðŸŒŸ Copy Task: ìž…ë ¥(src)ê³¼ ëª©í‘œ(tgt)ê°€ ë™ì¼í•¨\n",
        "    src = data\n",
        "    tgt = data\n",
        "\n",
        "    return src, tgt\n",
        "\n",
        "# ë§ˆìŠ¤í¬ ìƒì„± í•¨ìˆ˜ (íŒ¨ë”©ê³¼ ë¯¸ëž˜ í† í° ë°©ì§€)\n",
        "def generate_square_subsequent_mask(size):\n",
        "    # ë£©-ì–´í—¤ë“œ(Look-Ahead) ë§ˆìŠ¤í¬: ë””ì½”ë”ì—ì„œ ë¯¸ëž˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œ í•¨\n",
        "    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask"
      ],
      "metadata": {
        "id": "xfGrEFEZlj1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ëª¨ë¸ ì´ˆê¸°í™”\n",
        "model = TransformerModel(NTokens, EmbdSize, NHead, NHid, NLayers)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN) # PAD_TOKENì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ë¬´ì‹œ\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ë§ˆìŠ¤í¬ ì¤€ë¹„\n",
        "src_mask = generate_square_subsequent_mask(SeqLen)\n",
        "\n",
        "# 2. ê°„ë‹¨í•œ í•™ìŠµ ë£¨í”„ (100 ì—í­)\n",
        "print(\"--- í•™ìŠµ ì‹œìž‘ ---\")\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    src, tgt = generate_batch(BatchSize, SeqLen, NTokens)\n",
        "\n",
        "    # ðŸŒŸ PyTorch Transformer Encoderë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ, ìž…ë ¥(src)ê³¼ ëª©í‘œ(tgt)ê°€ ê°™ê³ \n",
        "    #    ë””ì½”ë” ë§ˆìŠ¤í¬ ëŒ€ì‹  ì¸ì½”ë” ë§ˆìŠ¤í¬ë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ (Copy Taskì´ë¯€ë¡œ) ë§ˆìŠ¤í¬ë¥¼ ë¬´ì‹œí•  ìˆ˜ ìžˆìŒ.\n",
        "    #    Seq2Seqë¥¼ êµ¬í˜„í•˜ë ¤ë©´ nn.Transformer() ì „ì²´ë¥¼ ì‚¬ìš©í•˜ê³  tgt_maskê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "    #    ì—¬ê¸°ì„œëŠ” Copy Task & nn.TransformerEncoder ì˜ˆì‹œì´ë¯€ë¡œ, srcë¥¼ ìž…ë ¥ë°›ì•„ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.\n",
        "\n",
        "    # nn.TransformerEncoderëŠ” Self-Attentionë§Œ ìˆ˜í–‰í•˜ë¯€ë¡œ, src_maskëŠ” íŒ¨ë”© ë§ˆìŠ¤í¬ ì—­í• ë§Œ í•˜ê±°ë‚˜ (ì—¬ê¸°ì„œëŠ” íŒ¨ë”© ì—†ìŒ),\n",
        "    # ì¼ë°˜ì ì¸ ì–¸ì–´ ëª¨ë¸ì˜ ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬ ì—­í• ì„ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
        "    # Copy Taskì´ë¯€ë¡œ ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "    output = model(src, src_mask)\n",
        "\n",
        "    # [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ í¬ê¸°, NTokens] -> [ë°°ì¹˜ í¬ê¸° * ì‹œí€€ìŠ¤ ê¸¸ì´, NTokens]\n",
        "    loss = criterion(output.reshape(-1, NTokens), tgt.reshape(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"--- í•™ìŠµ ì™„ë£Œ ---\")\n",
        "\n",
        "# 3. ê°„ë‹¨í•œ ì¶”ë¡  (Evaluation)\n",
        "model.eval()\n",
        "print(\"\\n--- ì¶”ë¡  ì‹œìž‘ (Copy Task) ---\")\n",
        "test_src, _ = generate_batch(1, SeqLen, NTokens) # ë°°ì¹˜ í¬ê¸° 1\n",
        "print(f\"ìž…ë ¥ ì‹œí€€ìŠ¤ (Source): \\n{test_src.squeeze().tolist()}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(test_src, src_mask[:test_src.size(0), :test_src.size(0)])\n",
        "\n",
        "    # ê°€ìž¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í† í°ì„ ì„ íƒí•˜ì—¬ ì˜ˆì¸¡ ì‹œí€€ìŠ¤ ìƒì„±\n",
        "    predicted_sequence = output.argmax(dim=-1).squeeze().tolist()\n",
        "\n",
        "print(f\"ì˜ˆì¸¡ ì‹œí€€ìŠ¤ (Predicted): \\n{predicted_sequence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHm6e6f-lxBh",
        "outputId": "026ce42a-0d76-41d3-9217-a30655e7604c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- í•™ìŠµ ì‹œìž‘ ---\n",
            "Epoch  20 | Loss: 0.0084\n",
            "Epoch  40 | Loss: 0.0018\n",
            "Epoch  60 | Loss: 0.0015\n",
            "Epoch  80 | Loss: 0.0011\n",
            "Epoch 100 | Loss: 0.0007\n",
            "--- í•™ìŠµ ì™„ë£Œ ---\n",
            "\n",
            "--- ì¶”ë¡  ì‹œìž‘ (Copy Task) ---\n",
            "ìž…ë ¥ ì‹œí€€ìŠ¤ (Source): \n",
            "[4, 6, 3, 8, 7]\n",
            "ì˜ˆì¸¡ ì‹œí€€ìŠ¤ (Predicted): \n",
            "[4, 6, 3, 8, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# ... (ì´ì „ ì½”ë“œì˜ ëª¨ë“  importì™€ í´ëž˜ìŠ¤ ì •ì˜ ìœ ì§€)\n",
        "# ... (TransformerModel, PositionalEncoding í´ëž˜ìŠ¤ ì •ì˜ ìœ ì§€)\n",
        "\n",
        "# ðŸŒŸ 1. ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜ ì¶”ê°€ ðŸŒŸ\n",
        "def calculate_accuracy(output, target, pad_idx):\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ì˜ ì¶œë ¥(output)ê³¼ ì‹¤ì œ ëª©í‘œ(target)ë¥¼ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "    íŒ¨ë”© í† í°(pad_idx)ì€ ê³„ì‚°ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    # output: [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ í¬ê¸°, NTokens]\n",
        "    # target: [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ í¬ê¸°]\n",
        "\n",
        "    # ì˜ˆì¸¡ëœ í´ëž˜ìŠ¤ ì¸ë±ìŠ¤ (ê°€ìž¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í† í°)\n",
        "    predicted_tokens = output.argmax(dim=-1) # [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ í¬ê¸°]\n",
        "\n",
        "    # íŒ¨ë”© í† í°ì´ ì•„ë‹Œ ìœ„ì¹˜ì— ëŒ€í•´ì„œë§Œ ë§ˆìŠ¤í¬ ìƒì„±\n",
        "    mask = (target != pad_idx)\n",
        "\n",
        "    # ì˜ˆì¸¡ì´ ì •í™•í•œ ìœ„ì¹˜ í™•ì¸\n",
        "    correct_predictions = (predicted_tokens == target)\n",
        "\n",
        "    # ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ì—¬ íŒ¨ë”©ì„ ì œì™¸í•œ ì •í™•í•œ ì˜ˆì¸¡ ìˆ˜ ì¹´ìš´íŠ¸\n",
        "    correct_sum = (correct_predictions & mask).sum().item()\n",
        "\n",
        "    # íŒ¨ë”©ì„ ì œì™¸í•œ ì „ì²´ í† í° ìˆ˜ ì¹´ìš´íŠ¸\n",
        "    total_tokens = mask.sum().item()\n",
        "\n",
        "    # ì •í™•ë„ ê³„ì‚°\n",
        "    accuracy = correct_sum / total_tokens if total_tokens > 0 else 0.0\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "QMRKdJcUsjlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ì´ì „ê³¼ ë™ì¼)\n",
        "PAD_TOKEN = 0  # Padding í† í° (ì†ì‹¤ ë° ì •í™•ë„ ê³„ì‚°ì—ì„œ ë¬´ì‹œí•  í† í°)\n",
        "NTokens = 10\n",
        "EmbdSize = 256\n",
        "NHead = 4\n",
        "NHid = 256\n",
        "NLayers = 2\n",
        "SeqLen = 5\n",
        "BatchSize = 32\n",
        "\n",
        "# ëª¨ë¸ ì´ˆê¸°í™” (ì´ì „ê³¼ ë™ì¼)\n",
        "# model, criterion, optimizer, src_mask ì •ì˜ (ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ)\n",
        "model = TransformerModel(NTokens, EmbdSize, NHead, NHid, NLayers)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN) # PAD_TOKENì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ë¬´ì‹œ\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "src_mask = generate_square_subsequent_mask(SeqLen)\n",
        "\n",
        "# ðŸŒŸ 3. ìˆ˜ì •ëœ í•™ìŠµ ë£¨í”„ ðŸŒŸ\n",
        "print(\"--- ëª¨ë¸ í•™ìŠµ ì‹œìž‘ (ì†ì‹¤ ë° ì •í™•ë„ ì¶œë ¥) ---\")\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    src, tgt = generate_batch(BatchSize, SeqLen, NTokens)\n",
        "\n",
        "    # ë§ˆìŠ¤í¬ í¬ê¸°ê°€ ë°°ì¹˜ë§ˆë‹¤ ë°”ë€” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë§ˆìŠ¤í¬ í¬ê¸° ì¡°ì • (Copy Taskì—ì„œëŠ” í•­ìƒ ê³ ì •)\n",
        "    current_src_mask = src_mask[:src.size(0), :src.size(0)]\n",
        "\n",
        "    output = model(src, current_src_mask)\n",
        "\n",
        "    # ì†ì‹¤ ê³„ì‚°\n",
        "    # output: [SeqLen, BatchSize, NTokens] -> [BatchSize * SeqLen, NTokens]\n",
        "    # tgt: [SeqLen, BatchSize] -> [BatchSize * SeqLen]\n",
        "    loss = criterion(output.reshape(-1, NTokens), tgt.reshape(-1))\n",
        "\n",
        "    # ðŸŒŸ ì •í™•ë„ ê³„ì‚°\n",
        "    accuracy = calculate_accuracy(output, tgt, PAD_TOKEN)\n",
        "\n",
        "    # ì—­ì „íŒŒ ë° ìµœì í™”\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"--- í•™ìŠµ ì™„ë£Œ ---\")\n",
        "\n",
        "# 4. ê°„ë‹¨í•œ ì¶”ë¡  (Evaluation) (ì´ì „ ì½”ë“œì™€ ë™ì¼)\n",
        "model.eval()\n",
        "print(\"\\n--- ì¶”ë¡  ì‹œìž‘ (Copy Task) ---\")\n",
        "test_src, _ = generate_batch(1, SeqLen, NTokens) # ë°°ì¹˜ í¬ê¸° 1\n",
        "print(f\"ìž…ë ¥ ì‹œí€€ìŠ¤ (Source): \\n{test_src.squeeze().tolist()}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(test_src, src_mask[:test_src.size(0), :test_src.size(0)])\n",
        "    predicted_sequence = output.argmax(dim=-1).squeeze().tolist()\n",
        "\n",
        "print(f\"ì˜ˆì¸¡ ì‹œí€€ìŠ¤ (Predicted): \\n{predicted_sequence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOzXkFRzsm5o",
        "outputId": "b2fc6bb9-a22f-4b09-c892-1d6e89020596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ëª¨ë¸ í•™ìŠµ ì‹œìž‘ (ì†ì‹¤ ë° ì •í™•ë„ ì¶œë ¥) ---\n",
            "Epoch  20 | Loss: 0.0056 | Accuracy: 1.0000\n",
            "Epoch  40 | Loss: 0.0020 | Accuracy: 1.0000\n",
            "Epoch  60 | Loss: 0.0010 | Accuracy: 1.0000\n",
            "Epoch  80 | Loss: 0.0011 | Accuracy: 1.0000\n",
            "Epoch 100 | Loss: 0.0008 | Accuracy: 1.0000\n",
            "--- í•™ìŠµ ì™„ë£Œ ---\n",
            "\n",
            "--- ì¶”ë¡  ì‹œìž‘ (Copy Task) ---\n",
            "ìž…ë ¥ ì‹œí€€ìŠ¤ (Source): \n",
            "[5, 5, 8, 6, 9]\n",
            "ì˜ˆì¸¡ ì‹œí€€ìŠ¤ (Predicted): \n",
            "[5, 5, 8, 6, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## í—ˆê¹…íŽ˜ì´ìŠ¤ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©(ê°€ìž¥ ì‹¤ìš©ì ì´ê³  ê°•ë ¥í•¨)\n",
        "\n",
        "* ëª¨ë¸ : í—ˆê¹…íŽ˜ì´ìŠ¤(Hugging Face) transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(ì˜ˆ:BERT, GPT-2) ë˜ëŠ” ê·¸ ê¸°ë°˜ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "* ìž¥ì \n",
        "   * ì‹¤ì œ ì‚°ì—…ì—ì„œ ê°€ìž¥ ë§Žì´ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ë°”ë¡œ ì‹¤ë¬´ì— ì ìš© ê°€ëŠ¥í•œ ê²°ê³¼ë¬¼ì„ ì–»ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.  \n",
        "   * ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì ì€ ë°ì´í„°ë¡œë„ ë†’ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. (íŒŒì¸íŠœë‹)\n",
        "\n",
        "* ë‹¨ì \n",
        "  * ìˆœìˆ˜í•œ íŠ¸ëžœìŠ¤í¬ë¨¸ ë…¼ë¬¸ì˜ ì•„í‚¤í…ì²˜ êµ¬í˜„ê³¼ëŠ” ê±°ë¦¬ê°€ ìžˆìœ¼ë©°, ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¢…ì†ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "VDYYjAEfnaIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate -U\n",
        "!pip install accelerate>=0.21.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6YFtRf_omYk",
        "outputId": "08121c2c-ab6c-400b-8a21-49259a77cf06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "\n",
        "# 1. ê°€ìƒ ë°ì´í„°ì…‹ ìƒì„± (ê¸ì •/ë¶€ì • í…ìŠ¤íŠ¸ ë¶„ë¥˜)\n",
        "data = {\n",
        "    'text': [\n",
        "        \"ì´ ì˜í™” ì •ë§ ìž¬ë¯¸ìžˆì—ˆì–´ìš”!\",\n",
        "        \"ë„ˆë¬´ ì§€ë£¨í•˜ê³  ì‹¤ë§ìŠ¤ëŸ¬ì› ìŠµë‹ˆë‹¤.\",\n",
        "        \"ìµœê³ ì˜ ì—°ê¸°, ë‹¤ì‹œ ë³´ê³  ì‹¶ë„¤ìš”.\",\n",
        "        \"ì‹œê°„ ë‚­ë¹„ì˜€ê³  ì¶”ì²œí•˜ê³  ì‹¶ì§€ ì•Šì•„ìš”.\",\n",
        "        \"ì˜¤ëžœë§Œì— ìˆ˜ìž‘ì„ ë§Œë‚¬ë„¤ìš”.\",\n",
        "        \"ê¸°ëŒ€ ì´í•˜ì˜€ê³  ë‚´ìš©ì´ ë¶€ì‹¤í•©ë‹ˆë‹¤.\"\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 1, 0] # 1: ê¸ì •, 0: ë¶€ì •\n",
        "}\n",
        "\n",
        "# Hugging Face Dataset ê°ì²´ë¡œ ë³€í™˜\n",
        "hf_dataset = Dataset.from_dict(data)\n",
        "\n",
        "# ë°ì´í„°ì…‹ì„ í•™ìŠµ(train)ê³¼ ê²€ì¦(eval) ì„¸íŠ¸ë¡œ ë¶„ë¦¬\n",
        "train_test_split = hf_dataset.train_test_split(test_size=0.3)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# 2. í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "# í•œêµ­ì–´ BERT ëª¨ë¸ì¸ 'monologg/kobert'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "MODEL_NAME = \"monologg/kobert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 3. ë°ì´í„° í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ ì •ì˜\n",
        "def tokenize_function(examples):\n",
        "    # max_length=128ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì œí•œí•˜ê³ , íŒ¨ë”©ê³¼ ìž˜ë¼ë‚´ê¸°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# 4. ë°ì´í„°ì…‹ì— í† í¬ë‚˜ì´ì§• ì ìš©\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# PyTorch í•™ìŠµì— í•„ìš” ì—†ëŠ” 'text' ì»¬ëŸ¼ì€ ì œê±°í•˜ê³ , 'input_ids', 'attention_mask', 'label'ë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"text\"])\n",
        "tokenized_eval_dataset = tokenized_eval_dataset.remove_columns([\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "dc07da7790ae466e8eb3665de735007f",
            "6e3c7bbc76b745a4bb8c89292437c03d",
            "a18c77eb018346208e3291f4e3fcbf3e",
            "eef895f983f24fe7818a776f0c26bf22",
            "fc8836fd22c0419192d09bad90ad439c",
            "a472a22a64b640a3a6ab47ef7e15d193",
            "af86f98289b2464e82c7933f14f1f072",
            "942752f2444c4decbcf5c9ba2bf20966",
            "e7a5a1b7b7554a299ced688548d7456d",
            "1b1b8f1faaf44af19842f9375e2952b7",
            "aa547a04a4f541468587fa5614cf1ed2",
            "d6729d9d8f4e47d9a7792f0946e60042",
            "c745b50183db455d9df4d36fcf0b48d5",
            "736ba1c3da644b3f9519fc3c6b617e3c",
            "8373a97c6b5d4959b6c4b673329cad10",
            "a8763fc044d24660948f3af96232bbd3",
            "39e5e1004a2544eba694fd4a762c389c",
            "2831879a63ea45879bc896964677ed03",
            "583bb5bc52d548a7be2865cf02513106",
            "a0f8ff5dd101404cb62bdc1f4e56b28f",
            "859ec16fb4fa4c57b0c94a3dc4769d32",
            "36f27c8bd52d46aebe2526afa7c110ca"
          ]
        },
        "id": "3nrsiyFWorhq",
        "outputId": "2b6eb87d-4a01-47d3-d2ef-0f18bb0af721"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository monologg/kobert contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/monologg/kobert .\n",
            " You can inspect the repository content at https://hf.co/monologg/kobert.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc07da7790ae466e8eb3665de735007f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6729d9d8f4e47d9a7792f0946e60042"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# 1. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (ì‹œí€€ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ ëª¨ë¸)\n",
        "# num_labels=2ëŠ” ê¸ì •/ë¶€ì • ë‘ ê°œì˜ í´ëž˜ìŠ¤ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "# 2. í‰ê°€ ì§€í‘œ í•¨ìˆ˜ ì •ì˜\n",
        "def compute_metrics(p):\n",
        "    # ì˜ˆì¸¡ëœ ë¡œì§“(logits)ì—ì„œ argmaxë¥¼ ì·¨í•˜ì—¬ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ êµ¬í•©ë‹ˆë‹¤.\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "\n",
        "    # ì •í™•ë„(Accuracy) ê³„ì‚°\n",
        "    accuracy = accuracy_score(p.label_ids, preds)\n",
        "\n",
        "    # ì •ë°€ë„, ìž¬í˜„ìœ¨, F1-score ê³„ì‚°\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='binary')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }\n",
        "\n",
        "# 3. TrainingArguments ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # ê²°ê³¼ë¬¼ ì €ìž¥ ê²½ë¡œ\n",
        "    num_train_epochs=5,              # ì´ í•™ìŠµ ì—í­ ìˆ˜\n",
        "    per_device_train_batch_size=8,   # ë””ë°”ì´ìŠ¤ë‹¹ í•™ìŠµ ë°°ì¹˜ í¬ê¸°\n",
        "    per_device_eval_batch_size=8,    # ë””ë°”ì´ìŠ¤ë‹¹ í‰ê°€ ë°°ì¹˜ í¬ê¸°\n",
        "    warmup_steps=500,                # ì›œì—… ìŠ¤í… ìˆ˜ (í•™ìŠµë¥  ì¡°ì •)\n",
        "    weight_decay=0.01,               # ê°€ì¤‘ì¹˜ ê°ì†Œ (ê·œì œí™”)\n",
        "    logging_dir='./logs',            # ë¡œê·¸ ì €ìž¥ ë””ë ‰í† ë¦¬\n",
        "    logging_steps=100,\n",
        "    # evaluation_strategy=\"epoch\",     # ë§¤ ì—í­ë§ˆë‹¤ í‰ê°€ ìˆ˜í–‰\n",
        "    # save_strategy=\"epoch\",           # ë§¤ ì—pochë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ìž¥\n",
        "    # load_best_model_at_end=True,     # í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœì  ëª¨ë¸ ë¡œë“œ\n",
        "    # metric_for_best_model=\"accuracy\" # ìµœì  ëª¨ë¸ ì„ íƒ ê¸°ì¤€\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "# 4. Trainer ì´ˆê¸°í™”\n",
        "trainer = Trainer(\n",
        "    model=model,                         # í›ˆë ¨í•  ëª¨ë¸\n",
        "    args=training_args,                  # í›ˆë ¨ ì¸ìž\n",
        "    train_dataset=tokenized_train_dataset, # í•™ìŠµ ë°ì´í„°ì…‹\n",
        "    eval_dataset=tokenized_eval_dataset,   # í‰ê°€ ë°ì´í„°ì…‹\n",
        "    compute_metrics=compute_metrics,     # í‰ê°€ ì§€í‘œ í•¨ìˆ˜\n",
        ")\n",
        "\n",
        "# 5. ëª¨ë¸ í•™ìŠµ ì‹œìž‘ (íŒŒì¸íŠœë‹)\n",
        "print(\"--- ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œìž‘ ---\")\n",
        "trainer.train()\n",
        "print(\"--- ëª¨ë¸ íŒŒì¸íŠœë‹ ì™„ë£Œ ---\")\n",
        "\n",
        "# 6. ìµœì¢… í‰ê°€\n",
        "print(\"\\n--- ìµœì¢… í‰ê°€ ê²°ê³¼ ---\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "lhQroq9SoueG",
        "outputId": "6a5c4b82-4559-473f-c304-e9588f75e0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œìž‘ ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:01, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ëª¨ë¸ íŒŒì¸íŠœë‹ ì™„ë£Œ ---\n",
            "\n",
            "--- ìµœì¢… í‰ê°€ ê²°ê³¼ ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.6737902164459229, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 0.0168, 'eval_samples_per_second': 119.167, 'eval_steps_per_second': 59.583, 'epoch': 5.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Is All You Need ë…¼ë¬¸ ê¸°ë°˜ í•µì‹¬ ëª¨ë“ˆ ì§ì ‘ êµ¬í˜„\n",
        "\n",
        "* ëª¨ë¸ : ë…¼ë¬¸(Attention is all you need) ì—ì„œ ì œì‹œí•œ í•µìŠ´ ëª¨ë“ˆ(Multi-Head Attention, Feed-Forward Network, Positional Encoding ë“±) ë§Œì„ ì§ì ‘ PyTorchë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "* ìž¥ì \n",
        "   * íŠ¸ëžœìŠ¤í¬ë¨¸ì˜ ë‚´ë¶€ êµ¬ì¡°ì™€ ë™ìž‘ ì›ë¦¬ë¥¼ ì™„ë²½í•˜ê²Œ íŒŒì•…í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
        "   * ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ì§ì ‘ ì œì–´í•  ìˆ˜ ìžˆì–´ ì»¤ìŠ¤í„°ë§ˆì´ì§•ì´ ìš©ì´í•©ë‹ˆë‹¤.\n",
        "\n",
        "* ë‹¨ì \n",
        "  * ì½”ë“œê°€ ê¸¸ê³ , ë””ë²„ê¹…ì´ í•„ìš”í•˜ë©°, í•™ìŠµ ì‹œê°„ì´ ê°€ìž¥ ì˜¤ëž˜ ê±¸ë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "```\n",
        "# ì½”ë“œë¡œ í˜•ì‹ ì§€ì •ë¨\n",
        "```\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "PRE04NFxoARM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    ë…¼ë¬¸ì—ì„œ ì œì‹œëœ ì‚¬ì¸ ë° ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ìœ„ì¹˜ ì¸ì½”ë”© êµ¬í˜„\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # pe: [max_len, 1, d_model] í¬ê¸°ì˜ ìœ„ì¹˜ ì¸ì½”ë”© í–‰ë ¬\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # position: [max_len, 1]\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # div_term: 10000^(-2i/d_model). iê°€ 0, 2, 4...ì¼ ë•Œì˜ ê°’.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # ì‚¬ì¸(ì§ìˆ˜ ì¸ë±ìŠ¤) ë° ì½”ì‚¬ì¸(í™€ìˆ˜ ì¸ë±ìŠ¤) ì ìš©\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # [max_len, d_model] -> [max_len, 1, d_model] (ë°°ì¹˜ ì°¨ì› ì¶”ê°€)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "        # ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµë˜ì§€ ì•Šë„ë¡ ë“±ë¡\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: ìž…ë ¥ ìž„ë² ë”©, [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ í¬ê¸°, d_model]\n",
        "        \"\"\"\n",
        "        # ìž…ë ¥ ìž„ë² ë”©ì— ìœ„ì¹˜ ì¸ì½”ë”© ê°’ ë”í•˜ê¸°\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "-iuCLG0slzTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    ë…¼ë¬¸ì˜ Scaled Dot-Product Attentionê³¼ Multi-Head ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_head, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_model // n_head # ê° í—¤ë“œì˜ ì°¨ì›\n",
        "\n",
        "        # Query, Key, Valueë¥¼ ìœ„í•œ ì„ í˜• ë ˆì´ì–´\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # ìµœì¢… ì¶œë ¥ì„ ìœ„í•œ ì„ í˜• ë ˆì´ì–´\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.d_k])) # ìŠ¤ì¼€ì¼ë§ ìƒìˆ˜\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "\n",
        "        # 1. Q, K, V ë³€í™˜ ë° í—¤ë“œ ë¶„í• \n",
        "        # Q, K, V: [ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, d_model]\n",
        "\n",
        "        # -> [ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, n_head, d_k] -> [ë°°ì¹˜ í¬ê¸°, n_head, ì‹œí€€ìŠ¤ ê¸¸ì´, d_k]\n",
        "        Q = self.w_q(query).view(query.shape[0], -1, self.n_head, self.d_k).transpose(1, 2)\n",
        "        K = self.w_k(key).view(key.shape[0], -1, self.n_head, self.d_k).transpose(1, 2)\n",
        "        V = self.w_v(value).view(value.shape[0], -1, self.n_head, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # 2. Scaled Dot-Product Attention ê³„ì‚°\n",
        "        # energy = Q @ K^T / sqrt(d_k)\n",
        "        # energy: [ë°°ì¹˜ í¬ê¸°, n_head, ì‹œí€€ìŠ¤ ê¸¸ì´, ì‹œí€€ìŠ¤ ê¸¸ì´]\n",
        "        energy = torch.matmul(Q, K.transpose(-2, -1)) / self.scale.to(Q.device)\n",
        "\n",
        "        # 3. ë§ˆìŠ¤í‚¹ ì ìš© (ì„ íƒ ì‚¬í•­)\n",
        "        if mask is not None:\n",
        "            # ë§ˆìŠ¤í¬ê°€ ì ìš©ë˜ëŠ” ìœ„ì¹˜ëŠ” -1e9 (ë§¤ìš° ìž‘ì€ ê°’)ë¡œ ì„¤ì •\n",
        "            energy = energy.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # 4. Softmax ì ìš©í•˜ì—¬ Attention Weights (ê°€ì¤‘ì¹˜) ê³„ì‚°\n",
        "        # attention: [ë°°ì¹˜ í¬ê¸°, n_head, ì‹œí€€ìŠ¤ ê¸¸ì´, ì‹œí€€ìŠ¤ ê¸¸ì´]\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "\n",
        "        # 5. Attention Weightsì™€ Value í–‰ë ¬ ê³±\n",
        "        # x: [ë°°ì¹˜ í¬ê¸°, n_head, ì‹œí€€ìŠ¤ ê¸¸ì´, d_k]\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "        # 6. í—¤ë“œ ê²°í•© (Concatenate Heads)\n",
        "        # x: [ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, n_head * d_k (d_model)]\n",
        "        x = x.transpose(1, 2).contiguous().view(query.shape[0], -1, self.d_model)\n",
        "\n",
        "        # 7. ìµœì¢… ì„ í˜• ë³€í™˜\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x, attention"
      ],
      "metadata": {
        "id": "HTEuffscxq-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    íŠ¸ëžœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì˜ ë‹¨ì¼ ë ˆì´ì–´ êµ¬ì„±\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_head, d_hid, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Self-Attention ì„œë¸Œ ë ˆì´ì–´\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_head, dropout)\n",
        "\n",
        "        # Feed-Forward ì„œë¸Œ ë ˆì´ì–´ (ê°„ì†Œí™”)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hid, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Layer Normalization ë° ë“œë¡­ì•„ì›ƒ\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "\n",
        "        # 1. Self-Attention ì„œë¸Œ ë ˆì´ì–´\n",
        "        # Residual connection + Layer Normalization\n",
        "        _src, _ = self.self_attn(src, src, src, src_mask)\n",
        "        src = self.norm1(src + self.dropout(_src))\n",
        "\n",
        "        # 2. Feed-Forward ì„œë¸Œ ë ˆì´ì–´\n",
        "        # Residual connection + Layer Normalization\n",
        "        _src = self.feed_forward(src)\n",
        "        src = self.norm2(src + self.dropout(_src))\n",
        "\n",
        "        return src"
      ],
      "metadata": {
        "id": "vN98GYuDxtoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (êµ¬í˜„í•˜ì‹  í´ëž˜ìŠ¤ì˜ __init__ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)\n",
        "d_model = 512    # ìž„ë² ë”© ì°¨ì›\n",
        "n_head = 8       # ë©€í‹° í—¤ë“œ ê°œìˆ˜\n",
        "d_hid = 2048     # Feed-Forward ë„¤íŠ¸ì›Œí¬ ì€ë‹‰ ì°¨ì›\n",
        "dropout = 0.1\n",
        "seq_len = 10     # ì‹œí€€ìŠ¤ ê¸¸ì´\n",
        "batch_size = 2   # ë°°ì¹˜ í¬ê¸°\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ê°€ìƒ ìž…ë ¥ ìž„ë² ë”© ìƒì„±\n",
        "# [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ í¬ê¸°, d_model] í˜•íƒœë¡œ ê°€ì • (Transformer êµ¬í˜„ í‘œì¤€)\n",
        "dummy_input = torch.randn(seq_len, batch_size, d_model)\n",
        "\n",
        "print(f\"ê°€ìƒ ìž…ë ¥ í¬ê¸°: {dummy_input.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEHtxRk2xv2r",
        "outputId": "86373555-d6eb-4177-b76a-13c9390336e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê°€ìƒ ìž…ë ¥ í¬ê¸°: torch.Size([10, 2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PositionalEncoding í´ëž˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "pe_module = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "# PE ëª¨ë“ˆ ì‹¤í–‰\n",
        "pe_output = pe_module(dummy_input)\n",
        "\n",
        "print(\"\\n--- PositionalEncoding í…ŒìŠ¤íŠ¸ ---\")\n",
        "print(f\"ìž…ë ¥ í¬ê¸°: {dummy_input.shape}\")\n",
        "print(f\"ì¶œë ¥ í¬ê¸°: {pe_output.shape}\")\n",
        "# PEê°€ ìž˜ ì ìš©ëëŠ”ì§€ í™•ì¸ (ìž…ë ¥ê³¼ ì¶œë ¥ì´ ë‹¬ë¼ì•¼ í•¨)\n",
        "print(f\"PE ì ìš© í›„ ìž…ë ¥ê³¼ ì¶œë ¥ì˜ ì°¨ì´ (L1 Norm): {torch.linalg.norm(dummy_input - pe_output):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpb4SisPx8T8",
        "outputId": "4daaed40-6eeb-40f7-efe8-6e33163a2ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PositionalEncoding í…ŒìŠ¤íŠ¸ ---\n",
            "ìž…ë ¥ í¬ê¸°: torch.Size([10, 2, 512])\n",
            "ì¶œë ¥ í¬ê¸°: torch.Size([10, 2, 512])\n",
            "PE ì ìš© í›„ ìž…ë ¥ê³¼ ì¶œë ¥ì˜ ì°¨ì´ (L1 Norm): 82.4915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MultiHeadAttention í´ëž˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "mha_module = MultiHeadAttention(d_model, n_head, dropout)\n",
        "\n",
        "# MHA ëª¨ë“ˆ ì‹¤í–‰ (Query, Key, Value ëª¨ë‘ dummy_input ì‚¬ìš©)\n",
        "# [ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, d_model] í˜•íƒœë¡œ ê°€ì •\n",
        "# ì‹¤ì œ êµ¬í˜„í•˜ì‹  MHAì˜ ìž…ë ¥ í˜•íƒœê°€ [ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, d_model]ë¡œ ê°€ì •í•©ë‹ˆë‹¤.\n",
        "# (ë§Œì•½ [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ í¬ê¸°, d_model]ì´ì—ˆë‹¤ë©´ .transpose(0, 1)ì´ í•„ìš”í•¨)\n",
        "dummy_input_mha = dummy_input.transpose(0, 1) # [ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, d_model]ë¡œ ë³€í™˜\n",
        "\n",
        "mha_output, attention_weights = mha_module(\n",
        "    dummy_input_mha, dummy_input_mha, dummy_input_mha\n",
        ")\n",
        "\n",
        "print(\"\\n--- MultiHeadAttention í…ŒìŠ¤íŠ¸ ---\")\n",
        "print(f\"ìž…ë ¥ í¬ê¸°: {dummy_input_mha.shape}\")\n",
        "print(f\"ì¶œë ¥ í¬ê¸°: {mha_output.shape}\")\n",
        "# ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì˜ í¬ê¸°: [ë°°ì¹˜ í¬ê¸°, n_head, ì‹œí€€ìŠ¤ ê¸¸ì´, ì‹œí€€ìŠ¤ ê¸¸ì´]\n",
        "print(f\"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í¬ê¸°: {attention_weights.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eF0pztDx9h5",
        "outputId": "7121bd97-3f00-4192-a44d-313235a93170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- MultiHeadAttention í…ŒìŠ¤íŠ¸ ---\n",
            "ìž…ë ¥ í¬ê¸°: torch.Size([2, 10, 512])\n",
            "ì¶œë ¥ í¬ê¸°: torch.Size([2, 10, 512])\n",
            "ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í¬ê¸°: torch.Size([2, 8, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TransformerEncoderLayer í´ëž˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "encoder_layer = TransformerEncoderLayer(d_model, n_head, d_hid, dropout)\n",
        "\n",
        "# ì¸ì½”ë” ë ˆì´ì–´ ì‹¤í–‰\n",
        "# ìž…ë ¥ í˜•íƒœ: [ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, d_model]\n",
        "encoder_output = encoder_layer(dummy_input_mha)\n",
        "\n",
        "print(\"\\n--- TransformerEncoderLayer í…ŒìŠ¤íŠ¸ ---\")\n",
        "print(f\"ìž…ë ¥ í¬ê¸°: {dummy_input_mha.shape}\")\n",
        "print(f\"ì¶œë ¥ í¬ê¸°: {encoder_output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPQI7LZLx-_g",
        "outputId": "733ce788-c097-4c69-c20e-1c8c3208a5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TransformerEncoderLayer í…ŒìŠ¤íŠ¸ ---\n",
            "ìž…ë ¥ í¬ê¸°: torch.Size([2, 10, 512])\n",
            "ì¶œë ¥ í¬ê¸°: torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FullTransformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, n_layers, n_head, d_hid, max_len, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # ðŸŒŸ êµ¬í˜„ í•„ìˆ˜ ìš”ì†Œ ðŸŒŸ\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len) # ì´ì „ì— êµ¬í˜„í•˜ì‹  PE ì‚¬ìš©\n",
        "\n",
        "        # ðŸŒŸ ì§ì ‘ êµ¬í˜„í•œ Encoder Layerë¥¼ ì—¬ëŸ¬ ì¸µìœ¼ë¡œ ìŒ“ìŠµë‹ˆë‹¤.\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, n_head, d_hid, dropout)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, n_layers)\n",
        "\n",
        "        # ðŸŒŸ ë””ì½”ë” êµ¬ì¡°ëŠ” ì¸ì½”ë”ì™€ ìœ ì‚¬í•˜ê²Œ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "        # ... (Cross-Attentionì´ í¬í•¨ëœ TransformerDecoderLayerê°€ í•„ìš”)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, trg_vocab_size) # ìµœì¢… ì¶œë ¥ ë ˆì´ì–´\n",
        "\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        # 1. ìž„ë² ë”© ë° Positional Encoding ì ìš©\n",
        "        # 2. Encoder ì‹¤í–‰ (src -> enc_src)\n",
        "        # 3. Decoder ì‹¤í–‰ (trg + enc_src -> output)\n",
        "        # 4. ìµœì¢… ì¶œë ¥ (output -> logit)\n",
        "        pass # ì—¬ê¸°ì— ì „ì²´ êµ¬í˜„ ì½”ë“œê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "0z4rNgFbyAnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# --- (í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜) ---\n",
        "SRC_VOCAB_SIZE = 100\n",
        "TRG_VOCAB_SIZE = 100\n",
        "D_MODEL = 512\n",
        "N_LAYERS = 3\n",
        "N_HEAD = 8\n",
        "D_HID = 2048\n",
        "MAX_LEN = 50\n",
        "DROPOUT = 0.1\n",
        "PAD_IDX = 0\n",
        "N_EPOCHS = 200\n",
        "\n",
        "# ðŸŒŸ Accuracy ê³„ì‚° í•¨ìˆ˜ (ì´ì „ì— PyTorch ë‚´ìž¥ íŠ¸ëžœìŠ¤í¬ë¨¸ì—ì„œ ì‚¬ìš©í–ˆë˜ ê²ƒê³¼ ìœ ì‚¬)\n",
        "def calculate_accuracy(output, target, pad_idx):\n",
        "    # output: [Seq Len, Batch Size, Vocab Size]\n",
        "    # target: [Seq Len, Batch Size]\n",
        "    predicted_tokens = output.argmax(dim=-1)\n",
        "    mask = (target != pad_idx)\n",
        "    correct_sum = (predicted_tokens == target).long().masked_select(mask).sum().item()\n",
        "    total_tokens = mask.sum().item()\n",
        "    return correct_sum / total_tokens if total_tokens > 0 else 0.0\n",
        "\n",
        "# ðŸŒŸ Copy Task ë°ì´í„° ìƒì„± í•¨ìˆ˜ (Seq Len=50, ë°°ì¹˜ í¬ê¸° 32)\n",
        "def generate_copy_batch(batch_size, seq_len, vocab_size):\n",
        "    # [Seq Len, Batch Size] í˜•íƒœë¡œ ìƒì„±\n",
        "    data = torch.randint(1, vocab_size, (seq_len, batch_size))\n",
        "    # Copy Taskì´ë¯€ë¡œ srcì™€ trgê°€ ë™ì¼\n",
        "    return data, data"
      ],
      "metadata": {
        "id": "fU81T-8-y2ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# --- (PositionalEncoding, MultiHeadAttention, TransformerEncoderLayer í´ëž˜ìŠ¤ê°€ ì—¬ê¸°ì— ì •ì˜ë˜ì–´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.) ---\n",
        "\n",
        "# íŽ¸ì˜ë¥¼ ìœ„í•´ ì—¬ê¸°ì— ê°„ì†Œí™”ëœ PositionalEncodingë§Œ ìž¬ì •ì˜í•©ë‹ˆë‹¤.\n",
        "# ì‹¤ì œ ì½”ë“œëŠ” ì´ì „ ë‹µë³€ì—ì„œ ì œê³µëœ ìƒì„¸ ì½”ë“œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0) # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: [Batch Size, Seq Len, d_model]\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "Tnc6ml4Ty7sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attentionì´ í¬í•¨ëœ TransformerDecoderLayerê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "# êµ¬í˜„ ë³µìž¡ë„ë¥¼ ë‚®ì¶”ê¸° ìœ„í•´ ì—¬ê¸°ì„œëŠ” PyTorch ë‚´ìž¥ Layerë¥¼ í™œìš©í•˜ì—¬ êµ¬ì¡°ë§Œ ì™„ì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "class FullTransformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, n_layers, n_head, d_hid, max_len, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # 1. ìž„ë² ë”© ë° Positional Encoding\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len)\n",
        "\n",
        "        # 2. Encoder Layer Stack\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, n_head, d_hid, dropout, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, n_layers)\n",
        "\n",
        "        # 3. Decoder Layer Stack\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, n_head, d_hid, dropout, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, n_layers)\n",
        "\n",
        "        # 4. ìµœì¢… ì¶œë ¥ ë ˆì´ì–´\n",
        "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder_embedding.weight, -initrange, initrange)\n",
        "        nn.init.uniform_(self.decoder_embedding.weight, -initrange, initrange)\n",
        "        nn.init.uniform_(self.fc_out.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.fc_out.bias)\n",
        "\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        # src, trg: [Batch Size, Seq Len]\n",
        "\n",
        "        # ìž„ë² ë”© + PE: [Batch Size, Seq Len, d_model]\n",
        "        src_embedded = self.pos_encoder(self.encoder_embedding(src) * math.sqrt(self.d_model))\n",
        "        trg_embedded = self.pos_encoder(self.decoder_embedding(trg) * math.sqrt(self.d_model))\n",
        "\n",
        "        # Encoder ì¶œë ¥ (Memory): [Batch Size, Seq Len, d_model]\n",
        "        enc_src = self.encoder(src_embedded, mask=src_mask)\n",
        "\n",
        "        # Decoder ì¶œë ¥: [Batch Size, Seq Len, d_model]\n",
        "        output = self.decoder(trg_embedded, enc_src, tgt_mask=trg_mask, memory_mask=src_mask)\n",
        "\n",
        "        # ìµœì¢… ì¶œë ¥: [Batch Size, Seq Len, Trg Vocab Size]\n",
        "        output = self.fc_out(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "bo0WxLcIzLZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# A. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì •\n",
        "# ----------------------------------------------------\n",
        "SRC_VOCAB_SIZE = 12   # 0: PAD, 1: BOS, 2: EOS, 3~11: ì‹¤ì œ í† í° (ì´ 12ê°œ)\n",
        "TRG_VOCAB_SIZE = 12\n",
        "D_MODEL = 256\n",
        "N_LAYERS = 3\n",
        "N_HEAD = 4\n",
        "D_HID = 512\n",
        "MAX_LEN = 10\n",
        "DROPOUT = 0.1\n",
        "PAD_IDX = 0\n",
        "N_EPOCHS = 200\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.0005\n",
        "\n",
        "BOS_IDX = 1\n",
        "EOS_IDX = 2\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# B. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
        "# ----------------------------------------------------\n",
        "\n",
        "def generate_copy_batch(batch_size, seq_len, vocab_size):\n",
        "    # ì‹¤ì œ í† í° ë²”ìœ„ (3 ~ vocab_size-1)\n",
        "    data = torch.randint(3, vocab_size, (batch_size, seq_len))\n",
        "    # Copy Taskì´ë¯€ë¡œ srcì™€ trgê°€ ë™ì¼\n",
        "    return data, data\n",
        "\n",
        "def create_mask(seq):\n",
        "    # Padding Mask (0ì¸ ìœ„ì¹˜ëŠ” True)\n",
        "    pad_mask = (seq == PAD_IDX)\n",
        "    return pad_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "def generate_square_subsequent_mask(size):\n",
        "    # Look-Ahead Mask (ë¯¸ëž˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œ í•¨)\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
        "    return mask\n",
        "\n",
        "def calculate_accuracy(output, target, pad_idx):\n",
        "    # output: [Batch Size, Seq Len, Vocab Size]\n",
        "    # target: [Batch Size, Seq Len]\n",
        "    predicted_tokens = output.argmax(dim=-1)\n",
        "\n",
        "    # íŒ¨ë”©ì´ ì•„ë‹Œ ìœ„ì¹˜ì— ëŒ€í•´ì„œë§Œ ë§ˆìŠ¤í¬ ìƒì„±\n",
        "    mask = (target != pad_idx)\n",
        "    correct_sum = (predicted_tokens == target).long().masked_select(mask).sum().item()\n",
        "    total_tokens = mask.sum().item()\n",
        "    return correct_sum / total_tokens if total_tokens > 0 else 0.0\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# C. ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ ë£¨í”„\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# 1. ëª¨ë¸ ì´ˆê¸°í™”\n",
        "model = FullTransformer(SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, D_MODEL, N_LAYERS, N_HEAD, D_HID, MAX_LEN, DROPOUT)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# 2. í•™ìŠµ ë£¨í”„\n",
        "print(\"--- Full Transformer í•™ìŠµ ì‹œìž‘ (ì§ì ‘ êµ¬í˜„ ê¸°ë°˜) ---\")\n",
        "model.train()\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    src_data, trg_data = generate_copy_batch(BATCH_SIZE, MAX_LEN, SRC_VOCAB_SIZE)\n",
        "\n",
        "    # ðŸŒŸ íƒ€ê²Ÿ ì‹œí€€ìŠ¤ ì¤€ë¹„: ë””ì½”ë” ìž…ë ¥(trg_input)ê³¼ ëª©í‘œ(trg_output) ë¶„ë¦¬\n",
        "    # Copy Taskì´ë¯€ë¡œ src=trgì´ì§€ë§Œ, Seq2SeqëŠ” ë””ì½”ë” ìž…ë ¥ê³¼ ëª©í‘œê°€ ë‹¤ë¦„\n",
        "    trg_input = trg_data[:, :-1]  # ë§ˆì§€ë§‰ í† í°(EOS) ì œì™¸\n",
        "    trg_output = trg_data[:, 1:]   # ì²« í† í°(BOS) ì œì™¸\n",
        "\n",
        "    # 3. ë§ˆìŠ¤í¬ ìƒì„±\n",
        "    src_mask = None # ì¸ì½”ë”ëŠ” ì¼ë°˜ì ìœ¼ë¡œ íŒ¨ë”© ë§ˆìŠ¤í¬ë§Œ ì‚¬ìš©í•˜ë‚˜, Copy TaskëŠ” íŒ¨ë”©ì„ ìƒëžµí•˜ì—¬ Noneìœ¼ë¡œ ë‘ .\n",
        "\n",
        "    # ë””ì½”ë”ì˜ ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬ (trg_inputì˜ ê¸¸ì´ë¡œ ìƒì„±)\n",
        "    trg_mask = generate_square_subsequent_mask(trg_input.size(1))\n",
        "\n",
        "    # 4. Forward Pass\n",
        "    output = model(src_data, trg_input, src_mask, trg_mask)\n",
        "\n",
        "    # output: [Batch Size, Seq Len-1, Trg Vocab Size]\n",
        "    # trg_output: [Batch Size, Seq Len-1]\n",
        "\n",
        "    # 5. ì†ì‹¤ ê³„ì‚° (í‰íƒ„í™” í›„)\n",
        "    loss = criterion(output.reshape(-1, output.shape[-1]), trg_output.reshape(-1))\n",
        "\n",
        "    # 6. ì •í™•ë„ ê³„ì‚°\n",
        "    accuracy = calculate_accuracy(output, trg_output, PAD_IDX)\n",
        "\n",
        "    # 7. Backward Pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Accuracy: {accuracy:.4f} | Time: {elapsed:.2f}s\")\n",
        "        start_time = time.time() # ì‹œê°„ ì´ˆê¸°í™”\n",
        "\n",
        "print(\"--- Full Transformer í•™ìŠµ ì™„ë£Œ ---\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# D. ìµœì¢… ì¶”ë¡  (Evaluation)\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# (ë³µìž¡ì„±ìœ¼ë¡œ ì¸í•´ ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë§Œ í¬í•¨í•©ë‹ˆë‹¤. ì‹¤ì œ ì¶”ë¡ ì€ ë””ì½”ë”© ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.)\n",
        "# print(\"\\n--- ìµœì¢… í‰ê°€ ê²°ê³¼: í•™ìŠµ ì •í™•ë„ ---\")\n",
        "# eval_loss = loss.item()\n",
        "# eval_accuracy = accuracy\n",
        "# print(f\"ìµœì¢… í•™ìŠµ ì†ì‹¤: {eval_loss:.4f}, ìµœì¢… í•™ìŠµ ì •í™•ë„: {eval_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6o4x7tazNO-",
        "outputId": "ee890b5f-7e59-4a21-ffbc-ba6d35224aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Full Transformer í•™ìŠµ ì‹œìž‘ (ì§ì ‘ êµ¬í˜„ ê¸°ë°˜) ---\n",
            "Epoch  20 | Loss: 1.9937 | Accuracy: 0.2326 | Time: 2.80s\n",
            "Epoch  40 | Loss: 1.8385 | Accuracy: 0.3056 | Time: 2.27s\n",
            "Epoch  60 | Loss: 1.4992 | Accuracy: 0.4236 | Time: 2.84s\n",
            "Epoch  80 | Loss: 1.0344 | Accuracy: 0.6076 | Time: 2.17s\n",
            "Epoch 100 | Loss: 0.5394 | Accuracy: 0.8229 | Time: 1.89s\n",
            "Epoch 120 | Loss: 0.2712 | Accuracy: 0.9097 | Time: 1.88s\n",
            "Epoch 140 | Loss: 0.2387 | Accuracy: 0.9236 | Time: 1.87s\n",
            "Epoch 160 | Loss: 0.0690 | Accuracy: 0.9757 | Time: 2.38s\n",
            "Epoch 180 | Loss: 0.1472 | Accuracy: 0.9583 | Time: 1.95s\n",
            "Epoch 200 | Loss: 0.0812 | Accuracy: 0.9792 | Time: 1.92s\n",
            "--- Full Transformer í•™ìŠµ ì™„ë£Œ ---\n"
          ]
        }
      ]
    }
  ]
}
